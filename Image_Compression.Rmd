---
title: "Image Compression using Principal Component Analysis"
author: "Nishit Jain"
date: "Built on: `r format(Sys.Date(), '%m/%d/%Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: true
    toc_float: true
    toc_depth: 4
  html_notebook:
    toc: true
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
# Load packages
required_packages <- c("tidyverse",
                       "jpeg",
                       "imager",
                       "raster",
                       "factoextra")

new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]

# Installing the missing packages
if (length(new_packages))
  install.packages(new_packages, repos='http://cran.us.r-project.org')

# Load all required packages
lapply(required_packages, library, character.only=T)

options(scipen=999)

source("./compression_utils.R")
```

# Objective

The objective of this notebook is:

* To document research on and implementation of Principal Component Analysis for image compression

* To demonstrate image compression using PCA

# Research

Principal Component Analysis is a way to identify and express patterns in data so as to highlight similarities and differences within it.

**This is extremely important for higher dimensional data which can not be visualized to identify patterns.**

After the pattern is identified, it can be used to compress the data to a lower dimension without much loss of information. This property of PCA is used for **image compression**.

PCA is based on eigendecomposition of covariance matrix of the data.

## Covariance

Covariance is a measure of the extent to which corresponding elements from two sets of ordered data move in the same direction.

Let $X$ and $Y$ be two random variables, then covariance is computed as:

$$
Cov(X, Y) = \frac{\sum_n{E[(X - E[X])(Y - E[Y])]}}{n-1}
$$

where $n$ is the number of items in the set

## Covariance Matrix

Covariance values for a set of variables are displayed as a covariance matrix, $C$, where

$$
C = \begin{bmatrix}
c_{1, 1} & c_{1, 2} & c_{1, 3} & ... & c_{1, n}\\
c_{2, 1} & c_{2, 2} & c_{2, 3} & ... & c_{2, n}\\
c_{3, 1} & c_{3, 2} & c_{3, 3} & ... & c_{3, n}\\
\vdots & \vdots & \vdots & ... & \vdots\\
c_{n, 1} & c_{n, 2} & c_{n, 3} & ... & c_{n, n}\\
\end{bmatrix}
$$

where $c_{i, j}$ is given as

$$
c_{i, j} = Cov(X_{i}, X_{j})
$$

**Note:** The diagonal elements, $c_{i, i}$ give $Var(X_{i})$

**Note:** Covariance matrix is symmetric as $c_{i, j} = c_{j, i}$

## Eigendecomposition

In linear algebra, eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors.

A vector $v$ of dimension $N$ is an **eigenvector** of a square $N Ã— N$ matrix $A$ if it satisfies the linear equation

$$
A\vec{v} = \lambda \vec{v}
$$
where $\lambda$ is any scalar value and corresponds to the **eigenvalue** for the respective **eigenvector**

**Note:** $\vec{v}$ needs to be a non-zero vector

Geometrically, this equation implies that for any matrix $A$, $\vec{v}$ represents any vector which, when linearly transformed using $A$, retains its direction but is scaled by a factor of $\lambda$.

## PCA Approach

PCA is performed using the following steps:

* Step 1: Represent the data as a $M X N$ matrix, where $M$ are the number of samples and $N$ are the number of features (dimensions)

* Step 2: Center the data by subtracting the mean of each feature across samples from the value of that feature for each sample

* Step 3: Calculate covariance matrix for the centered data

* Step 4: Perform eigendecomposition on the covariance matrix

* Step 5: Arrange the eigenvectors and eigenvalues in decreasing order of eigenvalues

* Step 6: Select top $l$ eigenvectors based on eigenvalues where $l < N$

## Image Compression

After selecting top $l$ eigenvectors based on eigenvalues of the covariance matrix of data, they can be used to compress images as follows:

* Step 1: Perform matrix multiplication of original data with selected eigenvectors to get lower dimension representation of data

* Step 2: Reconstruct original data using matrix multiplication of lower dimension data with inverse of matrix of eigenvectors

The reconstructed data will be a compressed version of original data

# Analysis

In this section we will demonstrate PCA based image compression on a real image.

## Load Image

We are using `jpeg::readJPEG()` function to load image from local directory.

```{r}
file_path <- "./sample_images/nishit_jain.jpeg"
loaded_image <- jpeg::readJPEG(file_path, native = FALSE)
plotImage(loaded_image)
```

## Image Properties

We are looking at the size and the dimensions of the original image.

```{r, echo=FALSE, results='asis'}
cat("* ", "Size of original image is ", file.info(file_path)$size, " bytes.\n\n", sep='')
cat("* ", "Dimensions of the image are: ", dim(loaded_image)[1], " X ", dim(loaded_image)[2], " X ", dim(loaded_image)[3], "\n\n", sep='')
```

The image is of size 400 X 400 with 3 channels (R, G, B)

## Image Channels

**Red Channel**

```{r, echo=FALSE}
loaded_image_copy <- as.array(loaded_image)

loaded_image_copy[, , 2] <- matrix(0L, nrow=dim(loaded_image_copy[, , 2])[1], ncol=dim(loaded_image_copy[, , 2])[2])
loaded_image_copy[, , 3] <- matrix(0L, nrow=dim(loaded_image_copy[, , 3])[1], ncol=dim(loaded_image_copy[, , 3])[2])

plotImage(loaded_image_copy)
```

**Green Channel**

```{r, echo=FALSE}
loaded_image_copy <- as.array(loaded_image)

loaded_image_copy[, , 1] <- matrix(0L, nrow=dim(loaded_image_copy[, , 1])[1], ncol=dim(loaded_image_copy[, , 1])[2])
loaded_image_copy[, , 3] <- matrix(0L, nrow=dim(loaded_image_copy[, , 3])[1], ncol=dim(loaded_image_copy[, , 3])[2])

plotImage(loaded_image_copy)
```

**Blue Channel**

```{r, echo=FALSE}
loaded_image_copy <- as.array(loaded_image)

loaded_image_copy[, , 1] <- matrix(0L, nrow=dim(loaded_image_copy[, , 1])[1], ncol=dim(loaded_image_copy[, , 1])[2])
loaded_image_copy[, , 2] <- matrix(0L, nrow=dim(loaded_image_copy[, , 2])[1], ncol=dim(loaded_image_copy[, , 2])[2])

plotImage(loaded_image_copy)
```

Since we have 3 channels (R, G, B), we will perform PCA for each channel individually.

## Principal Component Analysis

```{r}
# Extract 3 channels
loaded_image_r <- loaded_image[, , 1]
loaded_image_g <- loaded_image[, , 2]
loaded_image_b <- loaded_image[, , 3]
```

We will not center the value for each pixel to reconstruct the original image after compression.

```{r}
# PCA on each channel
loaded_image_r_pca <- stats::prcomp(loaded_image_r, center=FALSE)
loaded_image_g_pca <- stats::prcomp(loaded_image_g, center=FALSE)
loaded_image_b_pca <- stats::prcomp(loaded_image_b, center=FALSE)
```

```{r, echo=FALSE}
fviz_eig(loaded_image_r_pca)

fviz_pca_ind(loaded_image_r_pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```


# References

1. https://ourarchive.otago.ac.nz/bitstream/handle/10523/7534/OUCS-2002-12.pdf?sequence=1&isAllowed=y

2. https://stattrek.com/matrix-algebra/covariance-matrix.aspx

3. https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix

4. http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/

5. https://rpubs.com/aaronsc32/image-compression-principal-component-analysis